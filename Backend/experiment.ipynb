{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6149ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viditostwal/Desktop/RLM-Demo/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REPL + Oolong with Recursive LLM Calls (RLM)\n",
      "============================================================\n",
      "\n",
      "Loading dataset example 0...\n",
      "Question: Total number of rolls in this episode?\n",
      "Expected answer: 84\n",
      "Context length: 152,445 chars\n",
      "\n",
      "Connecting to: https://sergiopaniego-repl.hf.space\n",
      "Context loaded: 152,445 chars\n",
      "Available variables: ['context', 'FINAL', 'answer', 'safe_json_dumps', 'llm_query_batched', 'FINAL_VAR', 'format_exc', 'llm_query', 'llm_batch']\n",
      "\n",
      "--- Iteration 1 ---\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "Client error '402 Payment Required' for url 'https://router.huggingface.co/v1/chat/completions' (Request ID: Root=1-69668d2c-122dada14003a3510ab7db99;b9f53dc4-6624-49e0-bc84-84747ecb00ec)\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/402\n\nYou have reached the free monthly usage limit for fireworks-ai. Subscribe to PRO to get 20x more included usage, or add pre-paid credits to your account.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RLM-Demo/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:657\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RLM-Demo/.venv/lib/python3.12/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '402 Payment Required' for url 'https://router.huggingface.co/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/402",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 190\u001b[39m\n\u001b[32m    186\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✗ INCORRECT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, MAX_ITERATIONS + \u001b[32m1\u001b[39m):\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     response = \u001b[43mllm_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLLM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[:\u001b[32m400\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(response)\u001b[38;5;250m \u001b[39m>\u001b[38;5;250m \u001b[39m\u001b[32m400\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    134\u001b[39m     code_blocks = extract_code_blocks(response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mmain.<locals>.llm_chat\u001b[39m\u001b[34m(messages)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm_chat\u001b[39m(messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     71\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    LLM function for chat-style messages (outer loop),\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[33;03m    using HF Inference Providers.\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased for longer code responses\u001b[39;49;00m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RLM-Demo/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:933\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    905\u001b[39m parameters = {\n\u001b[32m    906\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    907\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    924\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    925\u001b[39m }\n\u001b[32m    926\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    927\u001b[39m     inputs=messages,\n\u001b[32m    928\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    931\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    932\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RLM-Demo/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:286\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    275\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.exit_stack.enter_context(\n\u001b[32m    276\u001b[39m         get_session().stream(\n\u001b[32m    277\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m         )\n\u001b[32m    285\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RLM-Demo/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:752\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    750\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    751\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: Client error '402 Payment Required' for url 'https://router.huggingface.co/v1/chat/completions' (Request ID: Root=1-69668d2c-122dada14003a3510ab7db99;b9f53dc4-6624-49e0-bc84-84747ecb00ec)\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/402\n\nYou have reached the free monthly usage limit for fireworks-ai. Subscribe to PRO to get 20x more included usage, or add pre-paid credits to your account."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "keepalive ping failed\n",
      "TimeoutError: timed out while closing connection\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/viditostwal/Desktop/RLM-Demo/.venv/lib/python3.12/site-packages/websockets/sync/connection.py\", line 784, in keepalive\n",
      "    with self.send_context():\n",
      "         ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/contextlib.py\", line 144, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/Users/viditostwal/Desktop/RLM-Demo/.venv/lib/python3.12/site-packages/websockets/sync/connection.py\", line 1020, in send_context\n",
      "    raise self.protocol.close_exc from original_exc\n",
      "websockets.exceptions.ConnectionClosedError: sent 1011 (internal error) keepalive ping timeout; no close frame received\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simple REPL + Oolong example with recursive LLM calls (RLM paradigm).\n",
    "\n",
    "Demonstrates the unified REPLEnv API that works with both remote servers\n",
    "and local execution using the same interface.\n",
    "\n",
    "Usage:\n",
    "    # Run against remote server\n",
    "    python examples/repl_oolong_simple.py\n",
    "\n",
    "    # Run locally (set SPACE_URL = None in the script)\n",
    "    python examples/repl_oolong_simple.py\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# HuggingFace token for Inference API\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "\n",
    "from repl_env import REPLEnv\n",
    "from repl_env.prompts import (\n",
    "    RLM_SYSTEM_PROMPT_QWEN,  # Use Qwen version (with cost warning)\n",
    "    QueryMetadata,\n",
    "    build_rlm_system_prompt,\n",
    "    build_user_prompt,\n",
    "    extract_code_blocks,\n",
    "    format_observation,\n",
    ")\n",
    "\n",
    "# ============== CONFIGURATION ==============\n",
    "# Set to None to run locally, or a URL to connect to remote Space\n",
    "SPACE_URL = \"https://sergiopaniego-repl.hf.space\"\n",
    "MODEL_NAME = \"Qwen/Qwen3-Coder-480B-A35B-Instruct\"\n",
    "DATASET_SUBSET = \"toy_dnd\"\n",
    "DATASET_SPLIT = \"validation\"\n",
    "EXAMPLE_INDEX = 0\n",
    "MAX_ITERATIONS = 30  # Paper uses 30\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"REPL + Oolong with Recursive LLM Calls (RLM)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load dataset\n",
    "    print(f\"\\nLoading dataset example {EXAMPLE_INDEX}...\")\n",
    "    dataset = load_dataset(\"oolongbench/oolong-real\", DATASET_SUBSET, split=DATASET_SPLIT)\n",
    "    example = dataset[EXAMPLE_INDEX]\n",
    "\n",
    "    context = example[\"context_window_text\"]\n",
    "    question = example[\"question\"]\n",
    "    expected = str(example[\"answer\"])\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Expected answer: {expected}\")\n",
    "    print(f\"Context length: {len(context):,} chars\")\n",
    "\n",
    "    # Load model for the outer loop (agent)\n",
    "    client = InferenceClient(\n",
    "        model=MODEL_NAME,\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "\n",
    "    def llm_chat(messages: list[dict]) -> str:\n",
    "        \"\"\"\n",
    "        LLM function for chat-style messages (outer loop),\n",
    "        using HF Inference Providers.\n",
    "        \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            max_tokens=2048,  # Increased for longer code responses\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    # Build task prompt (just the question, as per official RLM)\n",
    "    task_prompt = question\n",
    "\n",
    "    # Create environment - unified API for both local and remote!\n",
    "    if SPACE_URL:\n",
    "        print(f\"\\nConnecting to: {SPACE_URL}\")\n",
    "        env = REPLEnv(base_url=SPACE_URL)\n",
    "    else:\n",
    "        print(\"\\nRunning locally\")\n",
    "        # For local mode, provide LLM functions for llm_query/llm_query_batched support\n",
    "        def local_llm_query(prompt: str) -> str:\n",
    "            return llm_chat([{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "        def local_llm_batch(prompts: list[str]) -> list[str]:\n",
    "            return [local_llm_query(p) for p in prompts]\n",
    "\n",
    "        env = REPLEnv(llm_query_fn=local_llm_query, llm_batch_fn=local_llm_batch)\n",
    "\n",
    "    # Reset environment - same API for both local and remote\n",
    "    # Pass hf_token so the server uses our token for llm_query/llm_query_batched\n",
    "    result = env.reset(\n",
    "        context=context,\n",
    "        task_prompt=task_prompt,\n",
    "        max_iterations=MAX_ITERATIONS,\n",
    "        hf_token=HF_TOKEN,  # Server will use this token for sub-LLM calls\n",
    "    )\n",
    "    obs = result.observation\n",
    "\n",
    "    print(f\"Context loaded: {obs.context_length:,} chars\")\n",
    "    print(f\"Available variables: {obs.available_variables}\")\n",
    "\n",
    "    # Build initial messages (official RLM style):\n",
    "    # 1. System prompt\n",
    "    # 2. Assistant message with context metadata\n",
    "    # 3. User prompt with safeguard\n",
    "    query_metadata = QueryMetadata(\n",
    "        context_lengths=[obs.context_length],\n",
    "        context_total_length=obs.context_length,\n",
    "        context_type=\"str\",\n",
    "    )\n",
    "\n",
    "    messages = build_rlm_system_prompt(RLM_SYSTEM_PROMPT_QWEN, query_metadata)\n",
    "    messages.append(build_user_prompt(root_prompt=task_prompt, iteration=0))\n",
    "\n",
    "    # RLM loop\n",
    "    final_answer = None\n",
    "    for i in range(1, MAX_ITERATIONS + 1):\n",
    "        print(f\"\\n--- Iteration {i} ---\")\n",
    "\n",
    "        response = llm_chat(messages)\n",
    "        print(f\"LLM: {response[:400]}{'...' if len(response) > 400 else ''}\")\n",
    "\n",
    "        code_blocks = extract_code_blocks(response)\n",
    "        if not code_blocks:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "            messages.append({\"role\": \"user\", \"content\": \"Please provide code in ```repl``` blocks.\"})\n",
    "            continue\n",
    "\n",
    "        for code in code_blocks:\n",
    "            print(f\"\\nExecuting:\\n{code[:300]}{'...' if len(code) > 300 else ''}\")\n",
    "\n",
    "            # Execute code - same API for both local and remote!\n",
    "            result = env.execute(code)\n",
    "            obs = result.observation\n",
    "\n",
    "            print(f\"Success: {obs.result.success}\")\n",
    "            print(f\"Env iteration: {obs.iteration}/{obs.max_iterations}\")\n",
    "            if obs.result.stdout:\n",
    "                print(f\"Output: {obs.result.stdout[:300]}{'...' if len(obs.result.stdout) > 300 else ''}\")\n",
    "            if obs.result.stderr:\n",
    "                print(f\"Stderr: {obs.result.stderr[:200]}\")\n",
    "\n",
    "            if result.done:\n",
    "                state = env.state()\n",
    "                final_answer = state.final_answer\n",
    "                if final_answer:\n",
    "                    print(f\"\\n=== FINAL answer detected ===\")\n",
    "                else:\n",
    "                    print(f\"\\n=== Environment terminated (max iterations) ===\")\n",
    "                break\n",
    "\n",
    "        if result.done:\n",
    "            break  # Exit outer loop when env is done (with or without answer)\n",
    "\n",
    "        # Add assistant response and observation + next user prompt\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        observation_text = format_observation(obs)\n",
    "        next_prompt = build_user_prompt(root_prompt=task_prompt, iteration=i)\n",
    "        messages.append({\"role\": \"user\", \"content\": observation_text + \"\\n\\n\" + next_prompt[\"content\"]})\n",
    "\n",
    "    # Cleanup\n",
    "    env.close()\n",
    "\n",
    "    # Results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Got:      {final_answer}\")\n",
    "\n",
    "    if final_answer and str(final_answer).strip().lower() == expected.strip().lower():\n",
    "        print(\"✓ CORRECT!\")\n",
    "    else:\n",
    "        print(\"✗ INCORRECT\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2beca7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
