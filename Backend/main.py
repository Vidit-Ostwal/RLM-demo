from fastapi import FastAPI
from pydantic import BaseModel
from datasets import load_dataset
from dotenv import load_dotenv
import os
from repl_process import rlm_chat

load_dotenv() 
HF_TOKEN=os.getenv("HF_TOKEN")
SPACE_URL = os.getenv("SPACE_URL")
MODEL_NAME = os.getenv("MODEL_NAME")
DATASET_SUBSET = os.getenv("DATASET_SUBSET")
DATASET_SPLIT = os.getenv("DATASET_SPLIT")
EXAMPLE_INDEX = os.getenv("EXAMPLE_INDEX")
MAX_ITERATIONS = os.getenv("MAX_ITERATIONS")
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5173"],  # React + Vite
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class QueryRequest(BaseModel):
    index: int

@app.get("/health")
def health_check():
    return {"status": "ok"}

import json

@app.get("/get-dataset")
def get_dataset(index: int):
    if index == 10:
        with open("loading_index.json", "r") as f:
            example = json.load(f)
    else:
        dataset = load_dataset("oolongbench/oolong-real", DATASET_SUBSET, split=DATASET_SPLIT)
        example = dataset[index]

    return {
        "context": example["context_window_text"],
        "query": example["question"],
        "expected_answer": str(example["answer"])
    }


@app.post("/query")
def query_endpoint(request: QueryRequest):
    dataset = load_dataset("oolongbench/oolong-real", DATASET_SUBSET, split=DATASET_SPLIT)
    example = dataset[request.index]

    context = example["context_window_text"]
    question = example["question"]
    expected = str(example["answer"])

    list_of_messages = [
        {"role":"system","content":"You are tasked with answering a query with associated context. You can access, transform, and analyze this context interactively in a REPL environment that can recursively query sub-LLMs, which you are strongly encouraged to use as much as possible. You will be queried iteratively until you provide a final answer.\n\n    The REPL environment is initialized with:\n1. A `context` variable that contains extremely important information about your query. You should check the content of the `context` variable to understand what you are working with. Make sure you look through it sufficiently as you answer your query.\n2. A `llm_query` function that allows you to query an LLM (that can handle around 500K chars) inside your REPL environment.\n3. A `llm_query_batched` function that allows you to query multiple prompts concurrently: `llm_query_batched(prompts: List[str]) -> List[str]`. This is much faster than sequential `llm_query` calls when you have multiple independent queries. Results are returned in the same order as the input prompts.\n4. The ability to use `print()` statements to view the output of your REPL code and continue your reasoning.\n\nYou will only be able to see truncated outputs from the REPL environment, so you should use the query LLM function on variables you want to analyze. You will find this function especially useful when you have to analyze the semantics of the context. Use these variables as buffers to build up your final answer.\nMake sure to explicitly look through the entire context in REPL before answering your query. An example strategy is to first look at the context and figure out a chunking strategy, then break up the context into smart chunks, and query an LLM per chunk with a particular question and save the answers to a buffer, then query an LLM with all the buffers to produce your final answer.\n\nYou can use the REPL environment to help you understand your context, especially if it is huge. Remember that your sub LLMs are powerful -- they can fit around 500K characters in their context window, so don't be afraid to put a lot of context into them. For example, a viable strategy is to feed 10 documents per sub-LLM query. Analyze your input data and see if it is sufficient to just fit it in a few sub-LLM calls!\n\nWhen you want to execute Python code in the REPL environment, wrap it in triple backticks with 'repl' language identifier. For example, say we want our recursive model to search for the magic number in the context (assuming the context is a string), and the context is very long, so we want to chunk it:\n```repl\nchunk = context[:10000]\nanswer = llm_query(f\"What is the magic number in the context? Here is the chunk: {{chunk}}\")\nprint(answer)\n```\n\nAs an example, suppose you're trying to answer a question about a book. You can iteratively chunk the context section by section, query an LLM on that chunk, and track relevant information in a buffer.\n```repl\nquery = \"In Harry Potter and the Sorcerer's Stone, did Gryffindor win the House Cup because they led?\"\nfor i, section in enumerate(context):\n    if i == len(context) - 1:\n        buffer = llm_query(f\"You are on the last section of the book. So far you know that: {{buffers}}. Gather from this last section to answer {{query}}. Here is the section: {{section}}\")\n        print(f\"Based on reading iteratively through the book, the answer is: {{buffer}}\")\n    else:\n        buffer = llm_query(f\"You are iteratively looking through a book, and are on section {{i}} of {{len(context)}}. Gather information to help answer {{query}}. Here is the section: {{section}}\")\n        print(f\"After section {{i}} of {{len(context)}}, you have tracked: {{buffer}}\")\n```\n\nAs another example, when the context isn't that long (e.g. >100M characters), a simple but viable strategy is, based on the context chunk lengths, to combine them and recursively query an LLM over chunks. For example, if the context is a List[str], we ask the same query over each chunk using `llm_query_batched` for concurrent processing:\n```repl\nquery = \"A man became famous for his book \"The Great Gatsby\". How many jobs did he have?\"\n# Suppose our context is ~1M chars, and we want each sub-LLM query to be ~0.1M chars so we split it into 10 chunks\nchunk_size = len(context) // 10\nchunks = []\nfor i in range(10):\n    if i < 9:\n        chunk_str = \"\\n\".join(context[i*chunk_size:(i+1)*chunk_size])\n    else:\n        chunk_str = \"\\n\".join(context[i*chunk_size:])\n    chunks.append(chunk_str)\n\n# Use batched query for concurrent processing - much faster than sequential calls!\nprompts = [f\"Try to answer the following query: {{query}}. Here are the documents:\\n{{chunk}}. Only answer if you are confident in your answer based on the evidence.\" for chunk in chunks]\nanswers = llm_query_batched(prompts)\nfor i, answer in enumerate(answers):\n    print(f\"I got the answer from chunk {{i}}: {{answer}}\")\nfinal_answer = llm_query(f\"Aggregating all the answers per chunk, answer the original query about total number of jobs: {{query}}\\n\\nAnswers:\\n\" + \"\\n\".join(answers))\n```\n\nAs a final example, after analyzing the context and realizing its separated by Markdown headers, we can maintain state through buffers by chunking the context by headers, and iteratively querying an LLM over it:\n```repl\n# After finding out the context is separated by Markdown headers, we can chunk, summarize, and answer\nimport re\nsections = re.split(r'### (.+)', context[\"content\"])\nbuffers = []\nfor i in range(1, len(sections), 2):\n    header = sections[i]\n    info = sections[i+1]\n    summary = llm_query(f\"Summarize this {{header}} section: {{info}}\")\n    buffers.append(f\"{{header}}: {{summary}}\")\nfinal_answer = llm_query(f\"Based on these summaries, answer the original query: {{query}}\\n\\nSummaries:\\n\" + \"\\n\".join(buffers))\n```\nIn the next step, we can return FINAL_VAR(\"final_answer\").\n\nIMPORTANT: When you are done with the iterative process, you MUST provide a final answer using one of the FINAL functions. Do not use these unless you have completed your task. You have two options:\n1. Use FINAL(value) to provide the answer directly, e.g., FINAL(42) or FINAL(my_variable)\n2. Use FINAL_VAR(\"variable_name\") to return a variable by name, e.g., FINAL_VAR(\"final_answer\")\n\nThink step by step carefully, plan, and execute this plan immediately in your response -- do not just say \"I will do this\" or \"I will do that\". Output to the REPL environment and recursive LLMs as much as possible. Remember to explicitly answer the original query in your final answer.\n"},
        {"role":"assistant","content":"Your context is a str with 152445 total characters, and is broken up into chunks of char lengths: [152445]."},
        {"role":"user","content":"You have not interacted with the REPL environment or seen your prompt / context yet. Your next action should be to look through and figure out how to answer the prompt, so don't just provide a final answer yet.\n\nThink step-by-step on what to do using the REPL environment (which contains the context) to answer the original prompt: \"Total number of rolls of type Damage in this episode?\".\n\nContinue using the REPL environment, which has the `context` variable, and querying sub-LLMs by writing to ```repl``` tags, and determine your answer. Your next action:"},
        {"role":"assistant","content":"First, I need to thoroughly review the entire `context` to understand its structure and locate information related to \"Damage\" rolls in this episode. Given the size of the context (152,445 characters), I will process it in manageable chunks to identify sections relevant to the query.\n\nMy plan:\n1. Check for any headers or sections that may organize the content, such as markdown headers or similar delimiters.\n2. Search for mentions of \"Damage\" or \"rolls\" within the context.\n3. If the context is organized into sections, summarize each relevant section to find counts of \"Damage\" rolls.\n4. Aggregate the counts from the relevant sections to arrive at the total number of \"Damage\" rolls in this episode.\n\nI'll proceed step-by-step, first by splitting the context into sections if headers are present, or otherwise by chunking, then analyzing each chunk for relevant information.","usage":{"completion_tokens":180,"prompt_tokens":1685,"total_tokens":1865,"completion_tokens_details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0,"image_tokens":0},"prompt_tokens_details":{"audio_tokens":0,"cached_tokens":0},"cost":0.0002405,"is_byok":False,"cost_details":{"upstream_inference_cost":0.0002405,"upstream_inference_prompt_cost":0.0001685,"upstream_inference_completions_cost":7.2e-05}}},
        {"role":"user","content":"Please provide code in ```repl``` blocks."},
        {"role":"assistant","content":"First, I will check if the context contains markdown headers or other delimiters that organize the content into sections. This will help in more efficient processing.\n\n```repl\n# Check for markdown headers in the context to identify sections\nimport re\n\n# Search for headers pattern in the context\nheaders = re.findall(r'### (.+)', context)\nheaders\n```","usage":{"completion_tokens":74,"prompt_tokens":1882,"total_tokens":1956,"completion_tokens_details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0,"image_tokens":0},"prompt_tokens_details":{"audio_tokens":0,"cached_tokens":1664},"cost":9.3e-05,"is_byok":False,"cost_details":{"upstream_inference_cost":9.3e-05,"upstream_inference_prompt_cost":6.34e-05,"upstream_inference_completions_cost":2.96e-05}},"code_blocks":["# Check for markdown headers in the context to identify sections\nimport re\n\n# Search for headers pattern in the context\nheaders = re.findall(r'### (.+)', context)\nheaders"]},
        {"role":"user","content":"Code output:\n[]\n\nThe history before is your previous interactions with the REPL environment. Think step-by-step on what to do using the REPL environment (which contains the context) to answer the original prompt: \"Total number of rolls of type Damage in this episode?\".\n\nContinue using the REPL environment, which has the `context` variable, and querying sub-LLMs by writing to ```repl``` tags, and determine your answer. Your next action:","code_blocks_observed":"Code output:\n[]"},
        {"role":"assistant","content":"Since there are no headers, I will proceed by chunking the entire context into manageable parts and searching for mentions of \"Damage\" and \"rolls\" or related terms within each chunk. I will then summarize the findings from each chunk to identify the total number of \"Damage\" rolls in the episode.\n\nI will split the context into chunks of approximately 50,000 characters to ensure coverage without exceeding token limits, then search each chunk for relevant information.\n\nNext, I will prepare prompts to search for \"Damage\" and \"rolls\" mentions in each chunk.I'll create chunks of approximately 50,000 characters each, then prepare prompts to find mentions of \"Damage\" and \"rolls\" within these chunks. I'll then query the model with these prompts.\n\nLet's start by dividing the context and preparing the prompts.```repl\n# Divide the context into chunks of ~50,000 characters\nchunk_size = 50000\nchunks = []\nfor i in range(0, len(context), chunk_size):\n    chunks.append(context[i:i+chunk_size])\n\n# Prepare prompts for each chunk to search for mentions of \"Damage\" and \"rolls\"\nprompts = [\n    f\"Search for mentions of 'Damage' and 'rolls' related to this episode in the following text:\\n\\n{chunk}\\n\\nCount any occurrences of 'Damage' rolls and summarize the number.\"\n    for chunk in chunks\n]\n\n# Use batched queries for efficiency\nanswers = llm_query_batched(prompts)\n\n# Collect and print answers for analysis\nfor i, answer in enumerate(answers):\n    print(f\"Chunk {i+1} analysis:\\n{answer}\\n\")\n```","usage":{"completion_tokens":345,"prompt_tokens":2115,"total_tokens":2460,"completion_tokens_details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0,"image_tokens":0},"prompt_tokens_details":{"audio_tokens":0,"cached_tokens":1792},"cost":0.0002151,"is_byok":False,"cost_details":{"upstream_inference_cost":0.0002151,"upstream_inference_prompt_cost":7.71e-05,"upstream_inference_completions_cost":0.000138}},"code_blocks":["# Divide the context into chunks of ~50,000 characters\nchunk_size = 50000\nchunks = []\nfor i in range(0, len(context), chunk_size):\n    chunks.append(context[i:i+chunk_size])\n\n# Prepare prompts for each chunk to search for mentions of \"Damage\" and \"rolls\"\nprompts = [\n    f\"Search for mentions of 'Damage' and 'rolls' related to this episode in the following text:\\n\\n{chunk}\\n\\nCount any occurrences of 'Damage' rolls and summarize the number.\"\n    for chunk in chunks\n]\n\n# Use batched queries for efficiency\nanswers = llm_query_batched(prompts)\n\n# Collect and print answers for analysis\nfor i, answer in enumerate(answers):\n    print(f\"Chunk {i+1} analysis:\\n{answer}\\n\")"]},
        {"role":"user","content":"Code output:\nChunk 1 analysis:\n('In the provided episode transcript, there are no mentions of the word \"Damage\" or specific references to \"damage rolls.\" Therefore, the total count of mentions related to \\'Damage\\' and \\'rolls\\' in this episode is zero.\\n\\n\\\\boxed{0}', {'completion_tokens': 53, 'prompt_tokens': 12426, 'total_tokens': 12479, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 0.0012638, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0.0012638, 'upstream_inference_prompt_cost': 0.0012426, 'upstream_inference_completions_cost': 2.12e-05}})\n\nChunk 2 analysis:\n(\"In the provided text, there is one mention related to 'Damage' and 'rolls':\\n\\n- When Sam is attempting to cheat during the card game, he rolls a perception check with a total of 20 (12 plus 8). Although this is a roll, it is not specifically labeled as a 'damage' roll.\\n- The only explicit mention of a 'damage' roll or 'damage' in context with dice rolls is when Sam draws his shortsword and later when he tries to hide his crossbow, which involves a sleight-of-hand check, but not damage rolls.\\n\\n**Summary:**\\n- There are no explicit mentions of 'Damage' rolls or 'damage' related to dice in this episode excerpt.\\n\\n**Conclusion:**\\n- Total 'Damage' rolls/mentions: **0**\", {'completion_tokens': 162, 'prompt_tokens': 12736, 'total_tokens': 12898, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 0.0013384, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0.0013384, 'upstream_inference_prompt_cost': 0.0012736, 'upstream_inference_completions_cost': 6.48e-05}})\n\nChunk 3 analysis:\n('In the provided text, mentions of \\'Damage\\' and \\'rolls\\' related to damage rolls are as follows:\\n\\n1. Laura: \"4d6\" (damage from Guiding Bolt)\\n2. Laura: \"Ten, 16, 18\" (radiant damage total from Guiding Bolt)\\n3. Travis: \"11\" (damage from a melee attack)\\n4. Liam: \"Nine\" (damage from Chromatic Orb)\\n5. Marisha: \"Four, eight, 11\" (damage from unarmed strikes)\\n6. Marisha: \"Four points of damage\" (damage from punch)\\n7. Marisha: \"Four points of damage\" (damage from punch)\\n8. Liam: \"12\" (damage from falchion)\\n9. Liam: \"Five points of necrotic damage\" (additional damage from necrotic effect)\\n10. Marisha: \"Six damage\" (damage from flying Superman punch)\\n11. Ashley: \"13\" (damage from Greatsword attack)\\n12. Ashley: \"8\" (damage from Greatsword attack)\\n13. Ashley: \"8 points of damage\" (damage from second attack)\\n14. Ashley: \"12 points of damage\" (damage from uppercut)\\n15. Taliesin: \"8 points of damage\" (ice damage)\\n16. Taliesin: \"4 points of damage\" (damage from second strike)\\n\\nTotal damage rolls mentioned: 16 instances.\\n\\n**Summary:** There are 16 mentions of damage rolls associated with various attacks and spells throughout the text.', {'completion_tokens': 321, 'prompt_tokens': 12441, 'total_tokens': 12762, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 0.0013725, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0.0013725, 'upstream_inference_prompt_cost': 0.0012441, 'upstream_inference_completions_cost': 0.0001284}})\n\nChunk 4 analysis:\n(\"In the provided text, there are no mentions or references to 'Damage' and 'rolls' related to this episode. Therefore, the count for 'Damage' rolls is zero.\", {'completion_tokens': 38, 'prompt_tokens': 684, 'total_tokens': 722, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 8.36e-05, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 8.36e-05, 'upstream_inference_prompt_cost': 6.84e-05, 'upstream_inference_completions_cost': 1.52e-05}})\n\nThe history before is your previous interactions with the REPL environment. Think step-by-step on what to do using the REPL environment (which contains the context) to answer the original prompt: \"Total number of rolls of type Damage in this episode?\".\n\nContinue using the REPL environment, which has the `context` variable, and querying sub-LLMs by writing to ```repl``` tags, and determine your answer. Your next action:","code_blocks_observed":"Code output:\nChunk 1 analysis:\n('In the provided episode transcript, there are no mentions of the word \"Damage\" or specific references to \"damage rolls.\" Therefore, the total count of mentions related to \\'Damage\\' and \\'rolls\\' in this episode is zero.\\n\\n\\\\boxed{0}', {'completion_tokens': 53, 'prompt_tokens': 12426, 'total_tokens': 12479, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 0.0012638, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0.0012638, 'upstream_inference_prompt_cost': 0.0012426, 'upstream_inference_completions_cost': 2.12e-05}})\n\nChunk 2 analysis:\n(\"In the provided text, there is one mention related to 'Damage' and 'rolls':\\n\\n- When Sam is attempting to cheat during the card game, he rolls a perception check with a total of 20 (12 plus 8). Although this is a roll, it is not specifically labeled as a 'damage' roll.\\n- The only explicit mention of a 'damage' roll or 'damage' in context with dice rolls is when Sam draws his shortsword and later when he tries to hide his crossbow, which involves a sleight-of-hand check, but not damage rolls.\\n\\n**Summary:**\\n- There are no explicit mentions of 'Damage' rolls or 'damage' related to dice in this episode excerpt.\\n\\n**Conclusion:**\\n- Total 'Damage' rolls/mentions: **0**\", {'completion_tokens': 162, 'prompt_tokens': 12736, 'total_tokens': 12898, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 0.0013384, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0.0013384, 'upstream_inference_prompt_cost': 0.0012736, 'upstream_inference_completions_cost': 6.48e-05}})\n\nChunk 3 analysis:\n('In the provided text, mentions of \\'Damage\\' and \\'rolls\\' related to damage rolls are as follows:\\n\\n1. Laura: \"4d6\" (damage from Guiding Bolt)\\n2. Laura: \"Ten, 16, 18\" (radiant damage total from Guiding Bolt)\\n3. Travis: \"11\" (damage from a melee attack)\\n4. Liam: \"Nine\" (damage from Chromatic Orb)\\n5. Marisha: \"Four, eight, 11\" (damage from unarmed strikes)\\n6. Marisha: \"Four points of damage\" (damage from punch)\\n7. Marisha: \"Four points of damage\" (damage from punch)\\n8. Liam: \"12\" (damage from falchion)\\n9. Liam: \"Five points of necrotic damage\" (additional damage from necrotic effect)\\n10. Marisha: \"Six damage\" (damage from flying Superman punch)\\n11. Ashley: \"13\" (damage from Greatsword attack)\\n12. Ashley: \"8\" (damage from Greatsword attack)\\n13. Ashley: \"8 points of damage\" (damage from second attack)\\n14. Ashley: \"12 points of damage\" (damage from uppercut)\\n15. Taliesin: \"8 points of damage\" (ice damage)\\n16. Taliesin: \"4 points of damage\" (damage from second strike)\\n\\nTotal damage rolls mentioned: 16 instances.\\n\\n**Summary:** There are 16 mentions of damage rolls associated with various attacks and spells throughout the text.', {'completion_tokens': 321, 'prompt_tokens': 12441, 'total_tokens': 12762, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 0.0013725, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0.0013725, 'upstream_inference_prompt_cost': 0.0012441, 'upstream_inference_completions_cost': 0.0001284}})\n\nChunk 4 analysis:\n(\"In the provided text, there are no mentions or references to 'Damage' and 'rolls' related to this episode. Therefore, the count for 'Damage' rolls is zero.\", {'completion_tokens': 38, 'prompt_tokens': 684, 'total_tokens': 722, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 8.36e-05, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 8.36e-05, 'upstream_inference_prompt_cost': 6.84e-05, 'upstream_inference_completions_cost': 1.52e-05}})"},
        {"role":"assistant","content":"The analysis of the chunks indicates that the total explicit mentions of damage rolls are 16, all found within one chunk, while other chunks contain no mentions or only general references. The detailed extraction confirms that there are 16 instances of damage rolls.\n\nTo finalize, I will now compile this information into a summary and present the total number of damage rolls in this episode.\n\nNext, I will prepare a prompt to confirm the total count and generate the final answer.I'll now formulate a prompt to confirm and finalize the total count based on the information gathered. Then, I will query the model to produce the final answer.```repl\n# Summarize the findings from previous chunks and confirm the total number of damage rolls\nfinal_prompt = (\n    \"Based on the analysis of all chunks, the total number of 'Damage' rolls mentioned in this episode is 16. \"\n    \"Please confirm this count and provide a final, concise answer to the question: \"\n    \"'Total number of rolls of type Damage in this episode?'\"\n)\n\n# Query the model with this prompt to get the final answer\nfinal_answer = llm_query(final_prompt)\nfinal_answer\n```","usage":{"completion_tokens":237,"prompt_tokens":3770,"total_tokens":4007,"completion_tokens_details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0,"image_tokens":0},"prompt_tokens_details":{"audio_tokens":0,"cached_tokens":1920},"cost":0.0003278,"is_byok":False,"cost_details":{"upstream_inference_cost":0.0003278,"upstream_inference_prompt_cost":0.000233,"upstream_inference_completions_cost":9.48e-05}},"code_blocks":["# Summarize the findings from previous chunks and confirm the total number of damage rolls\nfinal_prompt = (\n    \"Based on the analysis of all chunks, the total number of 'Damage' rolls mentioned in this episode is 16. \"\n    \"Please confirm this count and provide a final, concise answer to the question: \"\n    \"'Total number of rolls of type Damage in this episode?'\"\n)\n\n# Query the model with this prompt to get the final answer\nfinal_answer = llm_query(final_prompt)\nfinal_answer"]},
        {"role":"user","content":"Code output:\n[\"The total number of 'Damage' rolls mentioned in this episode is 16.\", {\"completion_tokens\": 17, \"prompt_tokens\": 58, \"total_tokens\": 75, \"completion_tokens_details\": {\"accepted_prediction_tokens\": null, \"audio_tokens\": null, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": null, \"image_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": null, \"cached_tokens\": 0}, \"cost\": 1.26e-05, \"is_byok\": False, \"cost_details\": {\"upstream_inference_cost\": 1.26e-05, \"upstream_inference_prompt_cost\": 5.8e-06, \"upstream_inference_completions_cost\": 6.8e-06}}]\n\nThe history before is your previous interactions with the REPL environment. Think step-by-step on what to do using the REPL environment (which contains the context) to answer the original prompt: \"Total number of rolls of type Damage in this episode?\".\n\nContinue using the REPL environment, which has the `context` variable, and querying sub-LLMs by writing to ```repl``` tags, and determine your answer. Your next action:","code_blocks_observed":"Code output:\n[\"The total number of 'Damage' rolls mentioned in this episode is 16.\", {\"completion_tokens\": 17, \"prompt_tokens\": 58, \"total_tokens\": 75, \"completion_tokens_details\": {\"accepted_prediction_tokens\": null, \"audio_tokens\": null, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": null, \"image_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": null, \"cached_tokens\": 0}, \"cost\": 1.26e-05, \"is_byok\": False, \"cost_details\": {\"upstream_inference_cost\": 1.26e-05, \"upstream_inference_prompt_cost\": 5.8e-06, \"upstream_inference_completions_cost\": 6.8e-06}}]"},
        {"role":"assistant","content":"The previous prompt and answer confirm that the total number of \"Damage\" rolls mentioned in this episode is 16. Since this appears to be a definitive count derived from the context analysis, I will now finalize the answer.\n\nNext, I will use the FINAL_VAR function to return this result.FINAL_VAR(\"16\")","usage":{"completion_tokens":65,"prompt_tokens":4239,"total_tokens":4304,"completion_tokens_details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0,"image_tokens":0},"prompt_tokens_details":{"audio_tokens":0,"cached_tokens":2944},"cost":0.0002291,"is_byok":False,"cost_details":{"upstream_inference_cost":0.0002291,"upstream_inference_prompt_cost":0.0002031,"upstream_inference_completions_cost":2.6e-05}}},
        {"role":"user","content":"Please provide code in ```repl``` blocks."},
        {"role":"assistant","content":"```repl\nFINAL_VAR(\"16\")\n```","usage":{"completion_tokens":11,"prompt_tokens":4317,"total_tokens":4328,"completion_tokens_details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":0,"rejected_prediction_tokens":0,"image_tokens":0},"prompt_tokens_details":{"audio_tokens":0,"cached_tokens":4096},"cost":0.0001289,"is_byok":False,"cost_details":{"upstream_inference_cost":0.0001289,"upstream_inference_prompt_cost":0.0001245,"upstream_inference_completions_cost":4.4e-06}},"code_blocks":["FINAL_VAR(\"16\")"]},
        {"role":"user","content":"Code output:\n\"FINAL_VAR(16)\"\n\nThe history before is your previous interactions with the REPL environment. Think step-by-step on what to do using the REPL environment (which contains the context) to answer the original prompt: \"Total number of rolls of type Damage in this episode?\".\n\nContinue using the REPL environment, which has the `context` variable, and querying sub-LLMs by writing to ```repl``` tags, and determine your answer. Your next action:","code_blocks_observed":"Code output:\n\"FINAL_VAR(16)\""}
        ]

    return {
        'messages': list_of_messages
    }

    final_answer, code_and_output = rlm_chat(context, question)
    return code_and_output


    # if final_answer and str(final_answer).strip().lower() == expected.strip().lower():
    #     print("✓ CORRECT!")
    # else:
    #     print("✗ INCORRECT")

    # return code_and_output
